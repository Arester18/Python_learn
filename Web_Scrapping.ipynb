{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arester18/Python_learn/blob/main/Web_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web Scrapping"
      ],
      "metadata": {
        "id": "L9r7cobxXxG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Beautiful Soup"
      ],
      "metadata": {
        "id": "5DHBWpNMX2qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "HjMlD6e0-xXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EIYnYSi5j_v"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "website = \"https://subslikescript.com/movie/titanic-120338\"\n",
        "resultado = requests.get(website)\n",
        "contenido = resultado.text\n",
        "\n",
        "soup = BeautifulSoup(contenido, \"lxml\")\n",
        "#print(soup.prettify())\n",
        "\n",
        "box = soup.find(\"article\", class_ = \"main-article\")\n",
        "print(type(box))\n",
        "title = box.find(\"h1\").get_text()\n",
        "print(title)\n",
        "transcript = box.find(\"div\", class_ = \"full-script\").get_text(strip = True, separator = \" \")\n",
        "#print(transcript)\n",
        "\n",
        "with open(\"titanic.txt\", \"w\") as file:\n",
        "  file.write(transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpbRLqME6rSP",
        "outputId": "6568aac0-657e-49a1-d73c-2a4fc036a10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bs4.element.Tag'>\n",
            "Titanic (1997) - full transcript\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Selenium"
      ],
      "metadata": {
        "id": "Q2aEmHlqX7rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import Select\n",
        "import time\n",
        "\n",
        "website = \"https://www.adamchoi.co.uk/teamgoals/detailed\"\n",
        "path = \"\"\n",
        "\n",
        "driver = webdriver.Chrome(path)\n",
        "driver.get(website)\n",
        "\n",
        "#all_matches_button = driver.find_element_by_xpath(\"//*[@id=\"page-wrapper\"]/div/home-away-selector/div/div/div/div/label[2]\")\n",
        "all_matches_button = driver.find_element_by_xpath(\"//label[@analytics-event=\"All matches\"]\")\n",
        "all_matches_button.cliclk()\n",
        "\n",
        "\n",
        "#caja = driver.find_element_by_class_name(\"panel-body\")\n",
        "\n",
        "dropdown = Select(driver.find_element_by_id(\"country\"))\n",
        "dropdown.select_by_visible_text(\"Spain\")\n",
        "\n",
        "\n",
        "time.sleep(5)  #5 segundos\n",
        "\n",
        "matches = driver.find_elements_by_tag_name(\"tr\")\n",
        "partidos = []\n",
        "for matche in matches:\n",
        "  partidos.append(matche.text)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "df = pd.DataFrame({\"partidos\":partidos})\n",
        "print(df)\n",
        "df.to_csv(\"partidos.csv\", index = False)\n",
        "\n",
        "#Este código puede fallar cuando la página se demora demasiado en responder\n",
        "#lo anterior se puede solucionar con el sleep\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD66PaH9APWF",
        "outputId": "908c3372-3652-4c32-93a3-1807a10f9bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola mundo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scrapy"
      ],
      "metadata": {
        "id": "57vbsKpCYOPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abrir anaconda -> crear entorno virtual -> abrir terminal\n",
        "#ejecutar:conda install -c conda-forge scrapy\n",
        "#ejecutar: y\n",
        "\n",
        "#Para configurar anaconda con pycharm: ejecutar: where python ->para window\n",
        "#copiar ruta\n",
        "#abrir pycharm\n",
        "#abrir -> pegar ruta -> abrir\n",
        "#->preferencias o configuración ->proyecto ->interprete ->clic en rueda ->primera opcion\n",
        "#clic en conda environment -> ecisting environment -> interpreter\n",
        "#3 puntos -> pegar ruta ->ok->ok->ok\n",
        "\n",
        "#instalar libreria en cmd -> conda install -c conda-forge protego->enter->todo losto"
      ],
      "metadata": {
        "id": "0j0FywbgYWlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#abre anaconda -> abrir terminal del entorno virtual o abrir terminal en pycharm\n",
        "#scrapy                   #saca la lista de comandos disponibles\n",
        "#->ejecutar: scrapy bench  #analiza el rendimiento de web scrapy\n",
        "#->ejecutar: scrapy fetch http://google.com       #devuelve el código html\n",
        "#->ejecutar: scrapy genspider -> crea un nuevo spider usando una plantilla predefinida\n",
        "#setting ->muestra configuración\n",
        "#shell -> testea código\n",
        "#startproject-> crea un nuevo proyecto en scrapy\n",
        "#version -> muestra version de scrapy\n"
      ],
      "metadata": {
        "id": "R67wrM3ULd2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ejecutar: scrapy startproject spider_tutorial\n",
        "#middlewares.py -> se puede agregar funciones personalizadas\n",
        "#pipelines.py ->almacena la data que tenemos en una base de datos\n",
        "#setting ->agregar configuraciones"
      ],
      "metadata": {
        "id": "jg6tnVIeN_Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.worldometers.info/world-population/population-by-country/\n",
        "#->cd spider_tutorial/\n",
        "#->scrapy genspider worldometers www.worldometers.info/world-population/population-by-country\n",
        "#ya se creo el documento worldometers en spiders, ahora lo abrimos\n"
      ],
      "metadata": {
        "id": "V42BrLScPhp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class WorldometersSpider(scrapy.Spider):\n",
        "  name = \"worldometers\" #nombre del spyder\n",
        "  #links a los cuales se debe extraer data:\n",
        "  allowed_domains = [\"www.worldometers.info/\"]\n",
        "  #link exacto a la que le haremos web scrapping\n",
        "  start_urls = [\"https://www.worldometers.info/world-population/population-by-country/\"]\n",
        "\n",
        "  def parse(self, response):\n",
        "    pass"
      ],
      "metadata": {
        "id": "NZahzq48Qj6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plantillas mas usadas\n",
        "#1_ scrapy.Spider\n",
        "#2_ CrawlSpider ->spyder mas común para hacer crawlin\n",
        "#NOTA: Hacer crawlin no es lo mismo que web scrapping a un web site\n",
        "#Crawlin navega diversas páginas web con el objetivo de indexarlas\n",
        "#web scrapping: extración de datos"
      ],
      "metadata": {
        "id": "jNlnrHa5R7j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vamos a la página web y en el código html presionamos f1 y ponemos //h1\n",
        "#En CMD\n",
        "#Ejecutar\n",
        "->scrapy shell\n",
        ">>> r = scrapy.Request(url=\"https://www.worldometers.info/world-population/population-by-country/\")\n",
        ">>> fetch(r)\n",
        ">>> response.body() ->obtiene el documento html\n",
        ">>> response.xpath(\"//h1\")\n",
        ">>> response.xpath(\"//h1/text()\").get()\n",
        "#//td/a ->selecciona el nombre de los países\n",
        ">>> response.xpath(\"//td/a/text\").getall() #obtiene todos los países\n"
      ],
      "metadata": {
        "id": "ZA8O4JPLVvs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class WorldometersSpider(scrapy.Spider):\n",
        "  name = \"worldometers\" #nombre del spyder\n",
        "  #links a los cuales se debe extraer data:\n",
        "  allowed_domains = [\"www.worldometers.info/\"]\n",
        "  #link exacto a la que le haremos web scrapping\n",
        "  start_urls = [\"https://www.worldometers.info/world-population/population-by-country/\"]\n",
        "\n",
        "  def parse(self, response):\n",
        "    title = response.xpath(\"//h1/text()\").get()\n",
        "    countries = response.xpath(\"//td/a/text()\").getall()\n",
        "\n",
        "    yield{\n",
        "        \"title\":title,\n",
        "        \"countries\":countries,\n",
        "    }\n",
        "#vayan al terminal para correr el spyder, y tienen\n",
        "#estar en la carpeta que contiene scrapy.cfg\n",
        "#ejecutar: scrapy crawl worldometers\n"
      ],
      "metadata": {
        "id": "gHB8LCI0XKgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class WorldometersSpider(scrapy.Spider):\n",
        "  name = \"worldometers\" #nombre del spyder\n",
        "  #links a los cuales se debe extraer data:\n",
        "  allowed_domains = [\"www.worldometers.info/\"]\n",
        "  #link exacto a la que le haremos web scrapping\n",
        "  start_urls = [\"https://www.worldometers.info/world-population/population-by-country/\"]\n",
        "\n",
        "  def parse(self, response):\n",
        "    rows = response.xpath(\"//tr\")\n",
        "\n",
        "    for row in rows:\n",
        "      #title = response.xpath(\"//h1/text()\").get()\n",
        "      countries = row.xpath(\"./td/a/text()\").get()\n",
        "      population = row.xpath(\"./td[3]/text()\").get()\n",
        "\n",
        "      yield{\n",
        "          \"title\":title,\n",
        "          \"countries\":countries,\n",
        "          \"population\": population,\n",
        "      }\n",
        "#vayan al terminal para correr el spyder, y tienen\n",
        "#estar en la carpeta que contiene scrapy.cfg\n",
        "#exportar a archivo csv\n",
        "scrapy crawl worldometers -o population_1.csv\n",
        "#exportar a archivo json:\n",
        "scrapy crawl worldometers -o population_2.json"
      ],
      "metadata": {
        "id": "ny-JZpwNY04L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}